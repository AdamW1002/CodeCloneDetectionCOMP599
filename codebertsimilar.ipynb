{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamW1002/CodeCloneDetectionCOMP599/blob/main/codebertsimilar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbT3SApH0bn7"
      },
      "source": [
        "# Load required things and setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "84m73cj0zYi4",
        "outputId": "2dbde11c-842b-4e06-9573-2c8daa97f44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-xla==1.11\n",
            "  Using cached https://storage.googleapis.com/tpu-pytorch/wheels/cuda/112/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl (400.6 MB)\n",
            "Collecting cloud-tpu-client==0.10\n",
            "  Using cached cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting torch==1.11.0\n",
            "  Using cached torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Using cached google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "Collecting oauth2client\n",
            "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "Collecting typing-extensions\n",
            "  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Collecting google-auth>=1.4.1\n",
            "  Using cached google_auth-2.6.5-py2.py3-none-any.whl (156 kB)\n",
            "Collecting google-auth-httplib2>=0.0.3\n",
            "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting google-api-core<2dev,>=1.13.0\n",
            "  Using cached google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
            "Collecting six<2dev,>=1.6.1\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting uritemplate<4dev,>=3.0.0\n",
            "  Using cached uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting httplib2<1dev,>=0.9.2\n",
            "  Using cached httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
            "Collecting packaging>=14.3\n",
            "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
            "Collecting pytz\n",
            "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
            "Collecting setuptools>=40.3.0\n",
            "  Using cached setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n",
            "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
            "  Using cached googleapis_common_protos-1.56.0-py2.py3-none-any.whl (241 kB)\n",
            "Collecting requests<3.0.0dev,>=2.18.0\n",
            "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "Collecting google-auth>=1.4.1\n",
            "  Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "Collecting protobuf>=3.12.0\n",
            "  Using cached protobuf-3.20.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2\n",
            "  Using cached pyparsing-3.0.8-py3-none-any.whl (98 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "Collecting charset-normalizer~=2.0.0\n",
            "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Installing collected packages: pyasn1, urllib3, six, setuptools, rsa, pyparsing, pyasn1-modules, protobuf, idna, charset-normalizer, certifi, cachetools, requests, pytz, packaging, httplib2, googleapis-common-protos, google-auth, uritemplate, google-auth-httplib2, google-api-core, typing-extensions, oauth2client, google-api-python-client, torch-xla, torch, cloud-tpu-client\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.4.8\n",
            "    Uninstalling pyasn1-0.4.8:\n",
            "      Successfully uninstalled pyasn1-0.4.8\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.9\n",
            "    Uninstalling urllib3-1.26.9:\n",
            "      Successfully uninstalled urllib3-1.26.9\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 62.1.0\n",
            "    Uninstalling setuptools-62.1.0:\n",
            "      Successfully uninstalled setuptools-62.1.0\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.8\n",
            "    Uninstalling rsa-4.8:\n",
            "      Successfully uninstalled rsa-4.8\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.8\n",
            "    Uninstalling pyparsing-3.0.8:\n",
            "      Successfully uninstalled pyparsing-3.0.8\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1-modules 0.2.8\n",
            "    Uninstalling pyasn1-modules-0.2.8:\n",
            "      Successfully uninstalled pyasn1-modules-0.2.8\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.0\n",
            "    Uninstalling protobuf-3.20.0:\n",
            "      Successfully uninstalled protobuf-3.20.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.3\n",
            "    Uninstalling idna-3.3:\n",
            "      Successfully uninstalled idna-3.3\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.12\n",
            "    Uninstalling charset-normalizer-2.0.12:\n",
            "      Successfully uninstalled charset-normalizer-2.0.12\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2022.1\n",
            "    Uninstalling pytz-2022.1:\n",
            "      Successfully uninstalled pytz-2022.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: httplib2\n",
            "    Found existing installation: httplib2 0.20.4\n",
            "    Uninstalling httplib2-0.20.4:\n",
            "      Successfully uninstalled httplib2-0.20.4\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.56.0\n",
            "    Uninstalling googleapis-common-protos-1.56.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.56.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.35.0\n",
            "    Uninstalling google-auth-1.35.0:\n",
            "      Successfully uninstalled google-auth-1.35.0\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 3.0.1\n",
            "    Uninstalling uritemplate-3.0.1:\n",
            "      Successfully uninstalled uritemplate-3.0.1\n",
            "  Attempting uninstall: google-auth-httplib2\n",
            "    Found existing installation: google-auth-httplib2 0.1.0\n",
            "    Uninstalling google-auth-httplib2-0.1.0:\n",
            "      Successfully uninstalled google-auth-httplib2-0.1.0\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.31.5\n",
            "    Uninstalling google-api-core-1.31.5:\n",
            "      Successfully uninstalled google-api-core-1.31.5\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: oauth2client\n",
            "    Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.8.0\n",
            "    Uninstalling google-api-python-client-1.8.0:\n",
            "      Successfully uninstalled google-api-python-client-1.8.0\n",
            "  Attempting uninstall: torch-xla\n",
            "    Found existing installation: torch-xla 1.11\n",
            "    Uninstalling torch-xla-1.11:\n",
            "      Successfully uninstalled torch-xla-1.11\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "  Attempting uninstall: cloud-tpu-client\n",
            "    Found existing installation: cloud-tpu-client 0.10\n",
            "    Uninstalling cloud-tpu-client-0.10:\n",
            "      Successfully uninstalled cloud-tpu-client-0.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "earthengine-api 0.1.305 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.12 cloud-tpu-client-0.10 google-api-core-1.31.5 google-api-python-client-1.8.0 google-auth-1.35.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.56.0 httplib2-0.20.4 idna-3.3 oauth2client-4.1.3 packaging-21.3 protobuf-3.20.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.8 pytz-2022.1 requests-2.27.1 rsa-4.8 setuptools-62.1.0 six-1.16.0 torch-1.11.0 torch-xla-1.11 typing-extensions-4.1.1 uritemplate-3.0.1 urllib3-1.26.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "google",
                  "pkg_resources",
                  "setuptools",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/cuda/112/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl --force-reinstall "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0VZBFbTlCR-",
        "outputId": "cc614b47-a3e3-4c70-dab5-f01b4da9cff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rGU5scXqEbp",
        "outputId": "58f9c821-237f-4c6c-e3ce-c82b0da6faa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datetime import datetime\n",
        "import psutil \n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "from sklearn.metrics import f1_score\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-Ip-3YLpFJn",
        "outputId": "f550b3fc-31ac-4666-d52f-6f6d1e3e1c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xla:1\n",
            "USING TPU\n",
            "False\n",
            "xla:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "CHUNK_SIZE = 100000#data points per file\n",
        "MAX_TOKEN_DIM = 512 #controls padding and input to classifier\n",
        "LOAD_DATA = True\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  torch.cuda.device(device)\n",
        "try:\n",
        "  print(xm.xla_device())\n",
        "  device = xm.xla_device()\n",
        "  print(\"USING TPU\")\n",
        "except:\n",
        "  print(\"NOT USING TPU\")\n",
        "  pass\n",
        "print(torch.cuda.is_available())\n",
        "print(device)\n",
        "if torch.cuda.is_available():\n",
        "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "  print(\"using cuda\")\n",
        "  \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
        "codebert = AutoModel.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
        "torch.set_printoptions(precision=7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xhrTHlM6pOfa"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "  f = open(\"/content/drive/MyDrive/CloneData/data.jsonl\") #read sniipets and indices\n",
        "  entries = f.readlines()\n",
        "  objects = [json.loads(x) for x in entries] #load all functions\n",
        "  idx_to_function = dict() #id num -> code snippiet\n",
        " \n",
        "  for snippet in objects:#map to associate index to func\n",
        "    \n",
        "    idx_to_function[snippet[\"idx\"]] = snippet[\"func\"]\n",
        "\n",
        "  return idx_to_function #map id num to code 0 -> \"hello world\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UiomZTWIp0rQ"
      },
      "outputs": [],
      "source": [
        "def pairify_file(lines : list, idx_to_function : dict) -> tuple:\n",
        "  #list of lines id1 id2 label\n",
        "  # (code1, code2, label)\n",
        "  examples = [] #list of lines of \n",
        "  \n",
        "  for line in lines:\n",
        "    line_entries = line.replace(\"\\t\", \" \").split(\" \") #given line x y label, divide to find if x is y according to label\n",
        "    #print(line)\n",
        "    x = line_entries[0]\n",
        "    y = line_entries[1]\n",
        "    label = line_entries[2]\n",
        "    \n",
        "    examples.append((idx_to_function[x], idx_to_function[y], float(label))) #convert label to float for pytorch\n",
        "  return examples # [(\"hello world\", \"hi wurld\", 1) , (\"bye world\", \"EEEEEEE\", 0)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "49GUPqpspx2P"
      },
      "outputs": [],
      "source": [
        "def split_and_label_data(idx_to_function : dict): #convert pairs to useful training examples\n",
        "  return tuple(map(  lambda x : pairify_file(open(x).readlines(), idx_to_function)  , [\"/content/drive/MyDrive/CloneData/train.txt\",\"/content/drive/MyDrive/CloneData/test.txt\", \"/content/drive/MyDrive/CloneData/valid.txt\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EYrKogx0kQo"
      },
      "source": [
        "# Pre-calculate embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QLJLCUu4w0PY"
      },
      "outputs": [],
      "source": [
        "def embed(x : str) -> torch.TensorType:\n",
        "  with torch.no_grad():\n",
        "    code_tokens=tokenizer.tokenize(x)\n",
        "\n",
        "    if len(code_tokens) >= 510: #confirm tokes arent too big for model\n",
        "      return None\n",
        "    tokens=[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "\n",
        "    tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "    context_embeddings=codebert(torch.tensor(tokens_ids, device = device)[None,:])[0]\n",
        "    \n",
        "    flattened = torch.flatten(context_embeddings)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return flattened #torch.clamp(flattened, min = -2, max = 2) #return flattened embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ggTEK92rBaLQ"
      },
      "outputs": [],
      "source": [
        "def embed_data(data : list) -> list: #takes prog1, prog2, label and replaces prog with their embedding for every item in the list and filters out too long items\n",
        "  embedded_data = []\n",
        "  i = 0\n",
        "  \n",
        "  for x,y, label in data:\n",
        "\n",
        "    if i % 10 == 0:  \n",
        "      #print(\"using {} MB for {} of {}, embedded {}\".format(psutil.Process().memory_info().rss / (1024 * 1024),i, len(data), len(embedded_data)))\n",
        "      pass\n",
        "    emb_x = embed(x[0])\n",
        "    emb_y = embed(y[0])\n",
        "    #pdb.set_trace()\n",
        "    if emb_x != None and emb_y != None: #check code isnt too long\n",
        "      x_embed = emb_x #Standardize embeddings lengths since they depend on #of tokens\n",
        "      y_embed = emb_y\n",
        "     \n",
        "      padding_length_x  = (MAX_TOKEN_DIM * 768 - x_embed.size()[0])\n",
        "      padding_length_y  = (MAX_TOKEN_DIM * 768 - y_embed.size()[0])\n",
        "      \n",
        "      x_padded = torch.nn.functional.pad(x_embed, (int(padding_length_x/2), int(padding_length_x/2)))\n",
        "      y_padded = torch.nn.functional.pad(y_embed, (int(padding_length_y/2), int(padding_length_y/2)))\n",
        "      embedded_data.append((x_padded,y_padded, label))\n",
        "    i += 1\n",
        "  #print(f\"unique entries {len(set([embedded_data[0][0],  embedded_data[1][0]]  ))}\")\n",
        "  return embedded_data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "00CK6wwm9MjH"
      },
      "outputs": [],
      "source": [
        "class CloneDataset(Dataset): #dataset \n",
        "\n",
        "  def __init__(self,x : list ,y : list,labels : list):\n",
        "    assert len(x) == len(y) and len(y) == len(labels) #make sure all the same size\n",
        "    #standard boilerplate\n",
        "    self.x = (x)\n",
        "    self.y = (y)\n",
        "    self.labels = labels\n",
        "    self.length = len(x)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx], self.labels[idx]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hafzrTWspZ1P"
      },
      "outputs": [],
      "source": [
        "\n",
        "#test_data = embed_data(test_data)\n",
        "#validation_data = embed_data(validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Bz8fusDHb6S7"
      },
      "outputs": [],
      "source": [
        "def tokenize(code : str) -> list:\n",
        "  with torch.no_grad():\n",
        "    code_tokens=tokenizer.tokenize(code)\n",
        "    \n",
        "\n",
        "    if len(code_tokens) >= 510: #confirm tokes arent too big for model\n",
        "      return None\n",
        "    code_tokens += [tokenizer.pad_token] *  (510 - len(code_tokens)) #pad out to 510 which becomes 512\n",
        "    tokens=[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "\n",
        "    tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "    return torch.tensor(tokens_ids, device = device)[None,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I9GsM8wgAXRT"
      },
      "outputs": [],
      "source": [
        "def build_dataset(data : list):\n",
        "  x_list = []\n",
        "  y_list = []\n",
        "  label_list = []\n",
        "  i = 0\n",
        "  for x,y,l in data:#convert list of tuples to 3 separate lists\n",
        "    #x.to(device)\n",
        "    #y.to(device)\n",
        "    if i % 250 == 0:\n",
        "        print(f\"on data point {i}\")\n",
        "    x_tokens = tokenize(x)\n",
        "    y_tokens = tokenize(y)\n",
        "    if  not x_tokens is None and not y_tokens is None: #confirmm both seqs work\n",
        "      x_list.append(x_tokens)\n",
        "      y_list.append(y_tokens)\n",
        "      label_list.append(l)\n",
        "    i+=1\n",
        "\n",
        "  return CloneDataset(x_list, y_list, label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eZZu9vOmt8tm"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "       \n",
        "        \n",
        "        #A note on architecture for those interested, we eat CodeBERT embeddings of size X  * 768 which have been flattened\n",
        "        # Now those vectors are each fed into FF layer(s)\n",
        "        #Then they're concatnated and fed thru more FF layer(s)\n",
        "        # Then their dimensionality is shrunk down to 1, which is sigmoided\n",
        "        input_size = 256\n",
        "        layer2_size = 256\n",
        "        layer3_size = 128\n",
        "        layer4_size = 32\n",
        "        #self.xlayer_1 = nn.Linear(MAX_TOKEN_DIM * 768, layer2_size)\n",
        "        #self.ylayer_1 = nn.Linear(MAX_TOKEN_DIM * 768, layer2_size)\n",
        "        self.reduction = nn.Linear(MAX_TOKEN_DIM * 768, input_size)\n",
        "        self.ff1 = nn.Linear( 2 * input_size, layer2_size )\n",
        "        \n",
        "        self.ff2 = nn.Linear(layer2_size, 1)\n",
        "        #self.ff3 = nn.Linear(layer4_size, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.batchNorm1 = nn.BatchNorm1d(layer2_size, affine = False)\n",
        "\n",
        "        #nn.init.xavier_normal_(self.xlayer_1.weight)\n",
        "        #nn.init.xavier_normal_(self.ylayer_1.weight)\n",
        "        nn.init.xavier_normal_(self.ff1.weight)\n",
        "        nn.init.xavier_normal_(self.ff2.weight)\n",
        "        nn.init.xavier_normal_(self.reduction.weight)\n",
        "        #nn.init.xavier_normal_(self.ff3.weight)\n",
        "\n",
        "        #self.to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x,y):\n",
        "       x_reduced = self.reduction(x)   \n",
        "       y_reduced = self.reduction(y)       \n",
        "       combined = torch.cat((x_reduced, y_reduced),1)\n",
        "      \n",
        "       out = self.ff1(combined)\n",
        "       #print(f\"out is {out}\")\n",
        "       \n",
        "       out = self.sigmoid(out)\n",
        "       #xm.mark_step()\n",
        "       out = self.batchNorm1(out)\n",
        "       #xm.mark_step()\n",
        "       out = self.ff2(out)\n",
        "       #xm.mark_step()\n",
        "       #out = self.relu(out)\n",
        "       #out = self.ff3(out)\n",
        "       \n",
        "       #out = self.sigmoid(out)\n",
        "       return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk9DLArP5sWq",
        "outputId": "af3a6e2b-7103-4c51-81a0-82d2e9ffd32f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n"
          ]
        }
      ],
      "source": [
        "print(CHUNK_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save((torch.tensor([1,2,3]) ), \"test.pt\")\n",
        "torch.load(\"test.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlG-lUXbrIVM",
        "outputId": "acfe6fc5-be29-40bc-f033-867ad42025a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6LWC8lU0SGU",
        "outputId": "52ad6ac5-e847-4365-bb86-3a92acf4499e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading file 0\n",
            "xla:1\n",
            "took 0:00:05.843507 s\n",
            "loading file 1\n",
            "xla:1\n",
            "took 0:00:05.514958 s\n",
            "loading file 2\n",
            "xla:1\n",
            "took 0:00:05.564996 s\n",
            "loading file 3\n",
            "xla:1\n",
            "took 0:00:05.767646 s\n",
            "loading file 4\n",
            "xla:1\n",
            "took 0:00:05.826259 s\n",
            "loading file 5\n",
            "xla:1\n",
            "took 0:00:05.492077 s\n",
            "loading file 6\n",
            "xla:1\n",
            "took 0:00:05.520468 s\n",
            "loading file 7\n",
            "xla:1\n",
            "took 0:00:09.892807 s\n",
            "loading file 8\n",
            "xla:1\n",
            "took 0:00:07.063040 s\n",
            "loading file 9\n",
            "xla:1\n",
            "took 0:00:06.418474 s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "hector = False\n",
        "train_data = []\n",
        "if not LOAD_DATA:\n",
        "  #START\n",
        "  idx_to_function = load_data()\n",
        "  train_data, test_data,validation_data = split_and_label_data(idx_to_function)\n",
        "  if hector:\n",
        "    train_data = train_data[:100000]\n",
        "  i = 0\n",
        "  while i * CHUNK_SIZE < len(train_data):\n",
        "    relevant_data = train_data[i * CHUNK_SIZE: min(len(train_data), (i+1) * CHUNK_SIZE ) ] #get 100000 items at a time\n",
        "    print(i * CHUNK_SIZE, min(len(train_data), (i+1) * CHUNK_SIZE ), len(relevant_data))\n",
        "    print(f\"building dataset {i}\")\n",
        "    dataset = build_dataset(relevant_data) #get and save data\n",
        "    \n",
        "    print(f\"saving dataset{i}\")\n",
        "    torch.save(dataset, f\"train_data_{i}.pt\")\n",
        "    i+=1 #\n",
        "\n",
        "  #END\n",
        "elif LOAD_DATA:  \n",
        "  data_loaders = []\n",
        "  for i in range(10):#41 mins\n",
        "    print(f\"loading file {i}\")\n",
        "    start  = datetime.now()\n",
        "    print(device)\n",
        "    dl = torch.load(f\"/content/drive/MyDrive/CloneData/id_data/train_data_{0}.pt\", map_location= \"cpu\")\n",
        "    \n",
        "    data_loaders.append(dl)\n",
        "    end = datetime.now()\n",
        "    print(f\"took {(end-start)} s\")\n",
        "  train_data = data_loaders\n",
        "  train_data = torch.utils.data.ConcatDataset(data_loaders)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iUqf_FPNTSKa"
      },
      "outputs": [],
      "source": [
        "trainLoader = DataLoader(train_data, batch_size= 256, shuffle = False, drop_last = True)# BATCH SIZES MUST BE MULTIPLES OF 128\n",
        "all_train = DataLoader(train_data, batch_size= len(train_data), shuffle = False, drop_last = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKE-wWUmP201",
        "outputId": "da13740f-cacd-4fcf-dfad-bf1abfc5f4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "for x,y,l in all_train:\n",
        "  print(type(x),type(y), type(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W9hAA5MwQMIS"
      },
      "outputs": [],
      "source": [
        "#torch.save(train_data, \"train_data_0.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LCv4hn8xQTLa"
      },
      "outputs": [],
      "source": [
        "#del train_data\n",
        "#train_data = torch.load(\"train_data.pt\")\n",
        "#print(train_data, \"Fdsfasd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Nf1LORXo3kIz"
      },
      "outputs": [],
      "source": [
        "class F1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(F1, self).__init__()\n",
        "       \n",
        "\n",
        "\n",
        "    def forward(self, label,pred):\n",
        "      with torch.no_grad():\n",
        "        x = torch.round(pred) - label\n",
        "        x  = torch.flatten(x)\n",
        "        tp = torch.where(x == 0, 1, 0)\n",
        "        tp_count = float(torch.numel(torch.nonzero(tp)))\n",
        "\n",
        "        fp = torch.where(x == 1, 1, 0)\n",
        "        fp_count = float(torch.numel(torch.nonzero(fp)))\n",
        "\n",
        "        fn = torch.where(x == -1, 1, 0)\n",
        "        fn_count = float(torch.numel(torch.nonzero(fn)))\n",
        "        denom = (tp_count + .5 * (fp_count + fn_count))\n",
        "        #print(tp_count, fp_count, fn_count)\n",
        "        #print(denom)\n",
        "        if denom == 0:\n",
        "          return 0\n",
        "        return float(tp_count) / float(denom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXLGEVbK0qda"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mDmyDuz6wuN7"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  torch.cuda.empty_cache() \n",
        "  epochs  = 2 #standard boilerplate\n",
        "  model = Classifier()\n",
        "  print(model.ff1.weight.device)\n",
        "  #criterion = nn.BCELoss()\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "  #scorer = F1()\n",
        "  #scorer.to(device)\n",
        "\n",
        "  loss_history = []\n",
        "  f1_history = []\n",
        "  print(device)\n",
        "  model.to(device)\n",
        "  codebert.to(device)\n",
        "  for epoch in range(epochs): #standard training procedure\n",
        "    torch.cuda.empty_cache() \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    tp_count = 0 #setup for f1 score\n",
        "    fp_count = 0\n",
        "    fn_count = 0\n",
        "    f1 = 0\n",
        "    \n",
        "    j = 0\n",
        "    for x,y,label in trainLoader:\n",
        "      torch.cuda.empty_cache()  \n",
        "      \n",
        "\n",
        "     \n",
        "      #print(f\"b0 {batch[0]}\")\n",
        "      #print(f\"b1 {batch[1]}\")\n",
        "      #print(f\"b2 {batch[2]}\")\n",
        "\n",
        "      with torch.no_grad():\n",
        "        x = torch.reshape(x, (x.shape[0], x.shape[2])) # make Batch size X 1 X 512 into Batch size X 512\n",
        "        y = torch.reshape(y, (y.shape[0], y.shape[2]))\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "\n",
        "        \n",
        "        print(device, x.device, codebert.device)\n",
        "        #model.to(\"cpu\")\n",
        "        #codebert.to(device)\n",
        "        #torch.cuda.empty_cache()\n",
        "        #print(f\"before embedding codebert is on {codebert.device}\")\n",
        "        embed_start = datetime.now()\n",
        "        embedded_x=codebert(x)[0]\n",
        "       \n",
        "        embedded_y=codebert(y)[0]\n",
        "       \n",
        "        embed_end = datetime.now()\n",
        "        #codebert.to(\"cpu\")\n",
        "        #model.to(device)\n",
        "        #torch.cuda.empty_cache()\n",
        "        #print(embedded_x.shape)\n",
        "        #print(embedded_y.shape)\n",
        "        #embedded_x.to('cpu')\n",
        "        #embedded_y.to('cpu')\n",
        "        \n",
        "        embedded_x = torch.flatten(embedded_x, start_dim = 1)\n",
        "        embedded_y = torch.flatten(embedded_y, start_dim = 1)\n",
        "        #embedded_x.to(device)\n",
        "        #embedded_y.to(device)\n",
        "        #print(embedded_x.shape)\n",
        "        #print(embedded_y.shape)\n",
        "       \n",
        "      \n",
        "      model_start = datetime.now()\n",
        "      optimizer.zero_grad()\n",
        "      #print(embedded_x.dtype)\n",
        "      #print(\"max x {}\".format(torch.max(embedded_x)))\n",
        "      #print(f\"before prediction model is on device {model.ff1.weight.device}\")\n",
        "      pred = model(embedded_x,embedded_y)\n",
        "      \n",
        "     \n",
        "      #print(f\"pred is {pred.shape} {pred}\")\n",
        "     \n",
        "\n",
        "\n",
        "      #print(label.shape)\n",
        "      #print(pred.view(10).shape)\n",
        "      loss_start = datetime.now()\n",
        "      loss = criterion(torch.flatten(pred.unsqueeze(1)),torch.flatten(label.unsqueeze(1)))\n",
        "      #loss = torch.nn.functional.binary_cross_entropy_with_logits(torch.flatten(pred.unsqueeze(1)),torch.flatten(label.unsqueeze(1)))\n",
        "      loss.backward()\n",
        "      #nn.utils.clip_grad_norm_(model.parameters(), max_norm = 2.0, norm_type = 2.0)\n",
        "      optimizer.step()\n",
        "      xm.mark_step()\n",
        "      \n",
        "      loss_end = datetime.now()\n",
        "      #print(\"pred is {}\".format(pred))\n",
        "      epoch_loss += 0 #loss.item()\n",
        "      end = datetime.now()\n",
        "      model_delta_t = end-model_start \n",
        "      embed_delta_t = embed_end-embed_start\n",
        "      loss_delta_t = loss_start-loss_end\n",
        "     \n",
        "     \n",
        "      with torch.no_grad():\n",
        "        #f1_score = scorer(label,pred)\n",
        "        #F1_score = f1_score(label.cpu(), torch.round(pred).cpu())\n",
        "        #xm.mark_step()\n",
        "        score_start = datetime.now()\n",
        "        \n",
        "        #f1_history.append(F1_score)\n",
        "        if j % 50 == 0:\n",
        "          #print(torch.min(pred), torch.max(pred))\n",
        "          print(\"time per model iteration {} s\".format(model_delta_t.microseconds / 10**6))\n",
        "          print(\"time per embed iteration {} s\".format(embed_delta_t.microseconds / 10**6))\n",
        "          print(\"time per loss iteration {} s\".format(loss_delta_t.microseconds / 10**6))\n",
        "          loss_history.append(loss.item())\n",
        "          \n",
        "        \n",
        "          #calculate scores\n",
        "          pred_rounded = torch.round(pred)\n",
        "\n",
        "          for i in range(label.shape[0]):\n",
        "            #print(f\"i is {i}\")\n",
        "            if pred_rounded[i] == 1 and label[i] == 1:\n",
        "              tp_count += 1\n",
        "            elif pred_rounded[i] == 1 and label[i] == 0:\n",
        "              fp_count += 1\n",
        "            elif pred_rounded[i] == 0 and label[i] == 1:\n",
        "              fn_count += 1\n",
        "\n",
        "          if (tp_count + .5 * (fp_count + fn_count)) != 0: #dont get 0 for denom of f1\n",
        "            f1 = tp_count/(tp_count + .5 * (fp_count + fn_count))\n",
        "\n",
        "            #loss = loss.item.to('cpu')\n",
        "            #loss_history.append(loss.item())\n",
        "            f1_history.append(f1)\n",
        "            score_end = datetime.now()\n",
        "            print(f\"{(score_end-score_start)} s for scoring\")\n",
        "        j+=1\n",
        "  return (loss_history,f1_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsr8-zrrzR7_",
        "outputId": "ba9cf452-91cb-4bac-e39d-fa2d5cb283e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.165576 s\n",
            "time per embed iteration 0.091174 s\n",
            "time per loss iteration 0.839493 s\n",
            "0:00:06.755463 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.004469 s\n",
            "time per embed iteration 0.015867 s\n",
            "time per loss iteration 0.99609 s\n",
            "0:00:05.985564 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.003827 s\n",
            "time per embed iteration 0.014084 s\n",
            "time per loss iteration 0.996682 s\n",
            "0:00:05.941878 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.003724 s\n",
            "time per embed iteration 0.013195 s\n",
            "time per loss iteration 0.996771 s\n",
            "0:00:05.591883 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.004061 s\n",
            "time per embed iteration 0.014663 s\n",
            "time per loss iteration 0.996379 s\n",
            "0:00:05.760428 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.004077 s\n",
            "time per embed iteration 0.014206 s\n",
            "time per loss iteration 0.996412 s\n",
            "0:00:05.829251 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.0037 s\n",
            "time per embed iteration 0.011681 s\n",
            "time per loss iteration 0.996785 s\n",
            "0:00:05.388164 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.004615 s\n",
            "time per embed iteration 0.020311 s\n",
            "time per loss iteration 0.995921 s\n",
            "0:00:05.995060 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.004282 s\n",
            "time per embed iteration 0.014782 s\n",
            "time per loss iteration 0.996337 s\n",
            "0:00:05.554858 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.003941 s\n",
            "time per embed iteration 0.0133 s\n",
            "time per loss iteration 0.996586 s\n",
            "0:00:05.900073 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.004164 s\n",
            "time per embed iteration 0.012965 s\n",
            "time per loss iteration 0.996486 s\n",
            "0:00:05.692020 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.004109 s\n",
            "time per embed iteration 0.013294 s\n",
            "time per loss iteration 0.99648 s\n",
            "0:00:05.647465 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.004139 s\n",
            "time per embed iteration 0.014676 s\n",
            "time per loss iteration 0.996542 s\n",
            "0:00:05.351812 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.003889 s\n",
            "time per embed iteration 0.013238 s\n",
            "time per loss iteration 0.996632 s\n",
            "0:00:05.162570 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.003879 s\n",
            "time per embed iteration 0.013903 s\n",
            "time per loss iteration 0.996706 s\n",
            "0:00:05.398994 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.003666 s\n",
            "time per embed iteration 0.014696 s\n",
            "time per loss iteration 0.996821 s\n",
            "0:00:05.205402 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.00381 s\n",
            "time per embed iteration 0.013512 s\n",
            "time per loss iteration 0.996658 s\n",
            "0:00:05.449087 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.00431 s\n",
            "time per embed iteration 0.014751 s\n",
            "time per loss iteration 0.996322 s\n",
            "0:00:05.448974 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.007066 s\n",
            "time per embed iteration 0.013984 s\n",
            "time per loss iteration 0.993487 s\n",
            "0:00:05.612840 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "time per model iteration 0.003836 s\n",
            "time per embed iteration 0.012826 s\n",
            "time per loss iteration 0.996785 s\n",
            "0:00:05.564804 s for scoring\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n",
            "xla:1 xla:1 xla:1\n"
          ]
        }
      ],
      "source": [
        "loss_history, f1_history = train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-4-FxV90s_9"
      },
      "source": [
        "# Analzye results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hDwOnK2p_AY"
      },
      "outputs": [],
      "source": [
        "plt.scatter(list(range(len(f1_history))), f1_history)\n",
        "plt.title(\"F1 score\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"F1\")\n",
        "plt.show()\n",
        "plt.scatter(list(range(len(loss_history))), loss_history)\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "codebertsimilar.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}